{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoregressiveFlow:\n",
    "    \"\"\"\n",
    "    Implements a Masked Autoregressive Flow, which is a stack of mades such that the random numbers which drive made i\n",
    "    are generated by made i-1. The first made is driven by standard gaussian noise. In the current implementation, all\n",
    "    mades are of the same type. If there is only one made in the stack, then it's equivalent to a single made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_hiddens, act_fun, n_mades, batch_norm=False,\n",
    "                 input_order='sequential', mode='sequential', input=None):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        :param n_inputs: number of inputs\n",
    "        :param n_hiddens: list with number of hidden units for each hidden layer\n",
    "        :param act_fun: tensorflow activation function\n",
    "        :param n_mades: number of mades\n",
    "        :param batch_norm: whether to use batch normalization between mades\n",
    "        :param input_order: order of inputs of last made\n",
    "        :param mode: strategy for assigning degrees to hidden nodes: can be 'random' or 'sequential'\n",
    "        :param input: tensorflow placeholder to serve as input; if None, a new placeholder is created\n",
    "        \"\"\"\n",
    "\n",
    "        # save input arguments\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.act_fun = act_fun\n",
    "        self.n_mades = n_mades\n",
    "        self.batch_norm = batch_norm\n",
    "        self.momentum = momentum\n",
    "        self.mode = mode\n",
    "\n",
    "        self.input = tf.placeholder(dtype=dtype,shape=[None,n_inputs],name='x') if input is None else input\n",
    "        self.training = tf.placeholder_with_default(False,shape=(),name=\"training\")\n",
    "        self.parms = []\n",
    "\n",
    "        self.mades = []\n",
    "        self.bns = []\n",
    "        self.moments = []\n",
    "        self.assign_bns = []\n",
    "        self.u = self.input\n",
    "        self.logdet_dudx = 0.0\n",
    "\n",
    "        for i in range(n_mades):\n",
    "\n",
    "            # create a new made\n",
    "            made = mades.GaussianMade(n_inputs, n_hiddens, act_fun, input_order, mode, self.u)\n",
    "            self.mades.append(made)\n",
    "            self.parms += made.parms\n",
    "            # invert input order\n",
    "            input_order = input_order if input_order == 'random' else made.input_order[::-1]\n",
    "\n",
    "            # inverse autoregressive transform\n",
    "            self.u = made.u\n",
    "            self.logdet_dudx += 0.5 * tf.reduce_sum(made.logp, axis=1,keepdims=True)\n",
    "\n",
    "            # batch normalization\n",
    "            if batch_norm:\n",
    "                bn = BatchNormalization()\n",
    "                moments = tf.nn.moments(self.u,[0])\n",
    "                v_tmp = moments[1]\n",
    "                self.u = bn(self.u,training=self.training)\n",
    "                self.parms += [bn.loggamma,bn.beta]\n",
    "                v_tmp = tf.cond(self.training,lambda:v_tmp,lambda:bn.variance)\n",
    "                self.logdet_dudx += tf.reduce_sum(bn.loggamma) - 0.5 * tf.reduce_sum(tf.log(v_tmp+1e-5))\n",
    "                self.bns.append(bn)\n",
    "                self.moments.append(moments)\n",
    "                self.assign_bns.append(tf.assign(bn.mean,moments[0]))\n",
    "                self.assign_bns.append(tf.assign(bn.variance,moments[1]))\n",
    "\n",
    "        self.input_order = self.mades[0].input_order\n",
    "\n",
    "        # log likelihoods\n",
    "        self.L = tf.add(-0.5 * n_inputs * np.log(2 * np.pi) - 0.5 * tf.reduce_sum(self.u ** 2, axis=1,keepdims=True),\n",
    "                        self.logdet_dudx,name='L')\n",
    "\n",
    "        # train objective\n",
    "        self.trn_loss = -tf.reduce_mean(self.L,name='trn_loss')\n",
    "\n",
    "    def eval(self, x, sess, log=True, training=False):\n",
    "        \"\"\"\n",
    "        Evaluate log probabilities for given inputs.\n",
    "        :param x: data matrix where rows are inputs\n",
    "        :param sess: tensorflow session where the graph is run\n",
    "        :param log: whether to return probabilities in the log domain\n",
    "        :param training: in training, data mean and variance is used for batchnorm\n",
    "                         while outside training the saved mean and variance is used\n",
    "        :return: list of log probabilities log p(x)\n",
    "        \"\"\"        \n",
    "\n",
    "        lprob = sess.run(self.L,feed_dict={self.input:x,self.training:training})\n",
    "\n",
    "        return lprob if log else np.exp(lprob)\n",
    "    \n",
    "    def update_batch_norm(self,x,sess):\n",
    "        \"\"\"\n",
    "        Updates batch normalization moments with the values obtained in data set x.\n",
    "        :param x: data matrix whose moments will be used for the update\n",
    "        :param sess: tensorflow session where the graph is run\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        sess.run(self.assign_bns,feed_dict={self.input:x,self.training:True})\n",
    "        \n",
    "\n",
    "    def gen(self, sess, n_samples=1, u=None):\n",
    "        \"\"\"\n",
    "        Generate samples, by propagating random numbers through each made.\n",
    "        :param sess: tensorflow session where the graph is run\n",
    "        :param n_samples: number of samples\n",
    "        :param u: random numbers to use in generating samples; if None, new random numbers are drawn\n",
    "        :return: samples\n",
    "        \"\"\"\n",
    "\n",
    "        x = rng.randn(n_samples, self.n_inputs) if u is None else u\n",
    "\n",
    "        if getattr(self, 'batch_norm', False):\n",
    "\n",
    "            for made, bn in zip(self.mades[::-1], self.bns[::-1]):\n",
    "                x = bn.eval_inv(sess,x)\n",
    "                x = made.gen(sess,n_samples, x)\n",
    "\n",
    "        else:\n",
    "\n",
    "            for made in self.mades[::-1]:\n",
    "                x = made.gen(sess,n_samples, x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def calc_random_numbers(self, x, sess):\n",
    "        \"\"\"\n",
    "        Givan a dataset, calculate the random numbers used internally to generate the dataset.\n",
    "        :param x: numpy array, rows are datapoints\n",
    "        :param sess: tensorflow session where the graph is run\n",
    "        :return: numpy array, rows are corresponding random numbers\n",
    "        \"\"\"\n",
    "\n",
    "        return sess.run(self.u,feed_dict={self.input:x})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
